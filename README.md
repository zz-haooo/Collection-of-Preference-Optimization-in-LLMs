# Preference-Optimization-for-LLM-Alignment

A collection of Preference-Optimization methods in LLMs.

updating...

## Methods
<table class="tg">
<thead>
  <tr>
    <th class="tg-nrix" align="center" rowspan="2">Category</th>
    <th class="tg-baqh" align="center" rowspan="2">Method</th>
    <th class="tg-baqh" align="center" rowspan="2">Title</th>
    <th class="tg-0lax" align="center" rowspan="2">Venue</th>
    <th class="tg-baqh" align="center" rowspan="2">Release Date</th>
    <th class="tg-0lax" align="center" rowspan="2">Links</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix" align="center" rowspan="50">Offline</td>
    <td class="tg-0lax" align="center">DPO</td>
    <td class="tg-baqh" align="center">Direct preference optimization: Your language model is secretly a reward model</td>
    <td class="tg-0lax" align="center">NeurIPS 2023</td>
    <td class="tg-baqh" align="center">29 May 2023</td>
    <td class="tg-0lax" align="center"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">SLiC-HF</td>
    <td class="tg-baqh" align="center">Slic-hf: Sequence likelihood calibration with human feedback</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">17 May 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2305.10425">Paper</a></td>
  </tr>
   <tr>
    <td class="tg-0lax" align="center">RRHF</td>
    <td class="tg-baqh" align="center">RRHF: Rank Responses to Align Language Models with Human Feedback without tears</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">11 Apr 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2304.05302">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">PRO</td>
    <td class="tg-baqh" align="center">Preference ranking optimization for human alignment</td>
    <td class="tg-0lax" align="center">AAAI 2024</td>
    <td class="tg-baqh" align="center">30 Jun 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2306.17492v2">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">IPO</td>
    <td class="tg-baqh" align="center">A general theoretical paradigm to understand learning from human preferences</td>
    <td class="tg-0lax" align="center">AISTATS 2024</td>
    <td class="tg-baqh" align="center">18 Oct 2023</td>
    <td class="tg-0lax" align="center"><a href="https://proceedings.mlr.press/v238/gheshlaghi-azar24a/gheshlaghi-azar24a.pdf">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">CPO</td>
    <td class="tg-baqh" align="center">Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">16 Jan 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2401.08417">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">DPOP</td>
    <td class="tg-baqh" align="center">Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">20 Feb 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2402.13228">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">RSO</td>
    <td class="tg-baqh" align="center">Statistical Rejection Sampling Improves Preference Optimization</td>
    <td class="tg-0lax" align="center">ICLR 2024</td>
    <td class="tg-baqh" align="center">13 Sep 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2309.06657">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">KTO</td>
    <td class="tg-baqh" align="center">Kto: Model alignment as prospect theoretic optimization</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">2 Feb 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2402.01306">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">ORPO</td>
    <td class="tg-baqh" align="center">ORPO: Monolithic Preference Optimization without Reference Model</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">12 Mar 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2403.07691">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">length-regularized DPO</td>
    <td class="tg-baqh" align="center">Disentangling length from quality in direct preference optimization</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">28 Mar 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2403.19159">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">SimPO</td>
    <td class="tg-baqh" align="center">SimPO: Simple Preference Optimization with a Reference-Free Reward</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">23 May 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2405.14734">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">GPO</td>
    <td class="tg-baqh" align="center">Generalized Preference Optimization: A Unified Approach to Offline Alignment</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">8 Feb 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2402.05749">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">NPO</td>
    <td class="tg-baqh" align="center">Negative preference optimization: From catastrophic collapse to effective unlearning</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">8 Apr 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2404.05868">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">SPO</td>
    <td class="tg-baqh" align="center">A minimaximalist approach to reinforcement learning from human feedback</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">8 Jan 2024 </td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2401.04056">Paper</a></td>
  </tr>
   <tr>
    <td class="tg-0lax" align="center">DRO</td>
    <td class="tg-baqh" align="center">Offline Regularised Reinforcement Learning for Large Language Models Alignment</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">29 May 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2405.19107">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">RTO</td>
    <td class="tg-baqh" align="center">DPO Meets PPO: Reinforced Token Optimization for RLHF</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">29 Apr 2024 </td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2404.18922">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">Multi RSO</td>
    <td class="tg-baqh" align="center">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint</td>
    <td class="tg-0lax" align="center">ICLR Workshop 2024</td>
    <td class="tg-baqh" align="center">20 Apr 2024 </td>
    <td class="tg-0lax" align="center"><a href="https://weixiongust.github.io/WeiXiongUST/gshf.pdf">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">MODPO</td>
    <td class="tg-baqh" align="center">Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization</td>
    <td class="tg-0lax" align="center">ACL 2024</td>
    <td class="tg-baqh" align="center">5 Oct 2023 </td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2310.03708">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">A-LoL</td>
    <td class="tg-baqh" align="center">Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models</td>
    <td class="tg-0lax" align="center">ICLR 2024</td>
    <td class="tg-baqh" align="center">24 May 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2305.14718">Paper</a></td>
  </tr>



  </thead>
<tbody>
  <tr>
    <td class="tg-nrix" align="center" rowspan="50">Online</td>
    <td class="tg-0lax" align="center">RLHF-PPO (InstructGPT)</td>
    <td class="tg-baqh" align="center">Training language models to follow instructions with human feedback</td>
    <td class="tg-0lax" align="center">NeurIPS 2022</td>
    <td class="tg-baqh" align="center">4 Mar 2022</td>
    <td class="tg-0lax" align="center"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">OAIF</td>
    <td class="tg-baqh" align="center">Direct Language Model Alignment from Online AI Feedback</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">29 Feb 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2402.04792">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">RAFT</td>
    <td class="tg-baqh" align="center">RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment</td>
    <td class="tg-0lax" align="center">TMLR 2023</td>
    <td class="tg-baqh" align="center">13 Apr 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2304.06767">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">RLOO</td>
    <td class="tg-baqh" align="center">Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">22 Feb 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2402.14740">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">RLP</td>
    <td class="tg-baqh" align="center">Fine-Tuning Language Models with Reward Learning on Policy</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">28 Mar 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2403.19279">Paper</a></td>
  </tr>
</tbody>
</table>





## Analyzing and Understanding

<table class="tg">
<thead>
  <tr>
    <th class="tg-nrix" align="center" rowspan="2">Category</th>
    <th class="tg-baqh" align="center" rowspan="2">Title</th>
    <th class="tg-0lax" align="center" rowspan="2">Venue</th>
    <th class="tg-baqh" align="center" rowspan="2">Release Date</th>
    <th class="tg-0lax" align="center" rowspan="2">Links</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix" align="center" rowspan="50">Offline</td>
    <td class="tg-baqh" align="center">Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">6 Apr 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2404.04626">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-baqh" align="center">Understanding the Effects of RLHF on LLM Generalisation and Diversity</td>
    <td class="tg-0lax" align="center">ICLR2024</td>
    <td class="tg-baqh" align="center">10 Oct 2023</td>
    <td class="tg-0lax" align="center"><a href="https://openreview.net/pdf?id=PXD3FAVHJT">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-baqh" align="center">A Long Way To Go: Investigating Length Correlations in RLHF</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">5 Oct 2023</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2310.03716">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-baqh" align="center">Understanding the performance gap between online and offline alignment algorithms</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">14 May 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2405.08448">Paper</a></td>
  </tr>
  <tr>
    <td class="tg-baqh" align="center">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</td>
    <td class="tg-0lax" align="center">arxiv</td>
    <td class="tg-baqh" align="center">16 Apr 2024</td>
    <td class="tg-0lax" align="center"><a href="https://arxiv.org/pdf/2404.10719">Paper</a></td>
  </tr>
</tbody>
</table>
